{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2841 images belonging to 5 classes.\n",
      "Found 1078 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import ELU\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "\n",
    "# 5 классов(5 людей), размер фотографии 48х48\n",
    " \n",
    "num_classes = 5\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "\n",
    "train_data_dir = './faces/train/'\n",
    "validation_data_dir = './faces/validation/'\n",
    "\n",
    "\n",
    "\n",
    "# Data Augmentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      width_shift_range=0.4,\n",
    "      height_shift_range=0.4,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_32 (Conv2D)           (None, 48, 48, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,328,613\n",
      "Trainable params: 1,326,437\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Архитектура модели \n",
    "# 7 блоков\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "# 1-й блок\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
    "                 input_shape = (img_rows, img_cols, 3)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape = (img_rows, img_cols, 3)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# 2-й блок\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# 3-й блок\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# 4-й блок\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# 5-й блок\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# 6-й блок\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# 7-й блок\n",
    "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 177 steps, validate for 67 steps\n",
      "Epoch 1/10\n",
      "176/177 [============================>.] - ETA: 0s - loss: 1.0547 - accuracy: 0.6155\n",
      "Epoch 00001: val_loss improved from inf to 1.77262, saving model to test2_face_recognition_friends_vgg.h5\n",
      "177/177 [==============================] - 28s 156ms/step - loss: 1.0516 - accuracy: 0.6159 - val_loss: 1.7726 - val_accuracy: 0.4552\n",
      "Epoch 2/10\n",
      "176/177 [============================>.] - ETA: 0s - loss: 0.7041 - accuracy: 0.7544\n",
      "Epoch 00002: val_loss did not improve from 1.77262\n",
      "177/177 [==============================] - 27s 152ms/step - loss: 0.7033 - accuracy: 0.7547 - val_loss: 2.2645 - val_accuracy: 0.6856\n",
      "Epoch 3/10\n",
      "176/177 [============================>.] - ETA: 0s - loss: 0.6061 - accuracy: 0.7949\n",
      "Epoch 00003: val_loss improved from 1.77262 to 1.72874, saving model to test2_face_recognition_friends_vgg.h5\n",
      "177/177 [==============================] - 26s 146ms/step - loss: 0.6062 - accuracy: 0.7940 - val_loss: 1.7287 - val_accuracy: 0.5849\n",
      "Epoch 4/10\n",
      "176/177 [============================>.] - ETA: 0s - loss: 0.5069 - accuracy: 0.8309\n",
      "Epoch 00004: val_loss improved from 1.72874 to 1.62937, saving model to test2_face_recognition_friends_vgg.h5\n",
      "177/177 [==============================] - 26s 146ms/step - loss: 0.5076 - accuracy: 0.8304 - val_loss: 1.6294 - val_accuracy: 0.6670\n",
      "Epoch 5/10\n",
      "176/177 [============================>.] - ETA: 0s - loss: 0.5153 - accuracy: 0.8355\n",
      "Epoch 00005: val_loss improved from 1.62937 to 0.10121, saving model to test2_face_recognition_friends_vgg.h5\n",
      "177/177 [==============================] - 26s 146ms/step - loss: 0.5134 - accuracy: 0.8361 - val_loss: 0.1012 - val_accuracy: 0.9692\n",
      "Epoch 6/10\n",
      "176/177 [============================>.] - ETA: 0s - loss: 0.4788 - accuracy: 0.8469\n",
      "Epoch 00006: val_loss did not improve from 0.10121\n",
      "177/177 [==============================] - 26s 146ms/step - loss: 0.4780 - accuracy: 0.8471 - val_loss: 2.2219 - val_accuracy: 0.4925\n",
      "Epoch 7/10\n",
      "176/177 [============================>.] - ETA: 0s - loss: 0.4128 - accuracy: 0.8637\n",
      "Epoch 00007: val_loss did not improve from 0.10121\n",
      "177/177 [==============================] - 26s 146ms/step - loss: 0.4118 - accuracy: 0.8641 - val_loss: 0.4058 - val_accuracy: 0.8657\n",
      "Epoch 8/10\n",
      "176/177 [============================>.] - ETA: 0s - loss: 0.4040 - accuracy: 0.8683\n",
      "Epoch 00008: val_loss did not improve from 0.10121\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "177/177 [==============================] - 26s 146ms/step - loss: 0.4039 - accuracy: 0.8683 - val_loss: 0.5559 - val_accuracy: 0.7668\n",
      "Epoch 9/10\n",
      "176/177 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.9064\n",
      "Epoch 00009: val_loss improved from 0.10121 to 0.09202, saving model to test2_face_recognition_friends_vgg.h5\n",
      "177/177 [==============================] - 26s 147ms/step - loss: 0.2895 - accuracy: 0.9065 - val_loss: 0.0920 - val_accuracy: 0.9739\n",
      "Epoch 10/10\n",
      "176/177 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.9185\n",
      "Epoch 00010: val_loss did not improve from 0.09202\n",
      "177/177 [==============================] - 26s 147ms/step - loss: 0.2688 - accuracy: 0.9189 - val_loss: 0.2485 - val_accuracy: 0.9310\n"
     ]
    }
   ],
   "source": [
    "# TRAINING PART\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "                     \n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = ModelCheckpoint(\"test2_face_recognition_friends_vgg.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "# Learning Rate \n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "\n",
    "callbacks = [checkpoint, reduce_lr]\n",
    "\n",
    "\n",
    "# Компилируюм модель, Adam optimizer\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.01),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "nb_train_samples = 2841\n",
    "nb_validation_samples = 1078\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Aidyn', 1: 'Chandler', 2: 'Joey', 3: 'Pheobe', 4: 'Rachel'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Присвоение лэйблов\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка модели\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "classifier = load_model('test2_face_recognition_friends_vgg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "\n",
    "#5 классов \n",
    "face_classes = {0: 'Aidyn', 1: 'Chandler', 2: 'Joey', 3: 'Pheobe', 4: 'Rachel'}\n",
    "\n",
    "\n",
    "# Запись предикшена на видео\n",
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               font_scale=0.8, thickness=1):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    \n",
    "margin = 0.2\n",
    "img_size = 64\n",
    "\n",
    "\n",
    "\n",
    "#детектор лица\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "\n",
    "\n",
    "# Тестовое видео\n",
    "cap = cv2.VideoCapture('aaa.MOV')\n",
    "\n",
    "\n",
    "# Детекция лица на видео \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, None, fx=0.5, fy=0.5, interpolation = cv2.INTER_LINEAR)\n",
    "    preprocessed_faces = []           \n",
    " \n",
    "\n",
    "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w, _ = np.shape(input_img)\n",
    "    detected = detector(frame, 1)\n",
    "    faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "    \n",
    "    \n",
    "    if len(detected) > 0:\n",
    "        \n",
    "        for i, d in enumerate(detected):\n",
    "            \n",
    "            # Позиции лица и навешиваем прямоугольник \n",
    "            x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
    "            xw1 = max(int(x1 - margin * w), 0)\n",
    "            yw1 = max(int(y1 - margin * h), 0)\n",
    "            xw2 = min(int(x2 + margin * w), img_w - 1)\n",
    "            yw2 = min(int(y2 + margin * h), img_h - 1)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        \n",
    "        \n",
    "            #Encoding лица\n",
    "            face =  frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\n",
    "            face = cv2.resize(face, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "            face = face.astype(\"float\") / 255.0\n",
    "            face = img_to_array(face)\n",
    "            face = np.expand_dims(face, axis=0)\n",
    "            preprocessed_faces.append(face)\n",
    "\n",
    "        # prediction лица полученные выше\n",
    "        face_labels = []\n",
    "        for i, d in enumerate(detected):\n",
    "            preds = classifier.predict(preprocessed_faces[i])[0]\n",
    "            face_labels.append(face_classes[preds.argmax()])\n",
    "        \n",
    "        # Вставляем результаты в видео\n",
    "        for i, d in enumerate(detected):\n",
    "            label = \"{}\".format(face_labels[i])\n",
    "            draw_label(frame, (d.left(), d.top()), label)\n",
    "\n",
    "    cv2.imshow(\"Face Identifier\", frame)\n",
    "    if cv2.waitKey(1) == 13: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
